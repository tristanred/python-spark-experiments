{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/07 04:00:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/02/07 04:00:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:00:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:00:57 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:01:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:01:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:01:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:01:57 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:02:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:02:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:02:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:02:57 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:03:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:03:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:03:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:03:57 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:04:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "ERROR:root:Exception while sending command.                         (0 + 0) / 2]\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "RuntimeError: reentrant call inside <_io.BufferedReader name=66>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/bitnami/python/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"/opt/bitnami/spark/python/pyspark/context.py\", line 292, in signal_handler\n",
      "    self.cancelAllJobs()\n",
      "  File \"/opt/bitnami/spark/python/pyspark/context.py\", line 1195, in cancelAllJobs\n",
      "    self._jsc.sc().cancelAllJobs()\n",
      "  File \"/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\", line 334, in get_return_value\n",
      "    raise Py4JError(\n",
      "py4j.protocol.Py4JError: An error occurred while calling o13.sc\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 503, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/workspaces/sparkTest/development.ipynb Cell 1'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B643a5c50726f675c5347445f576f726b5c737061726b54657374/workspaces/sparkTest/development.ipynb#ch0000000vscode-remote?line=11'>12</a>\u001b[0m   x, y \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandom(), random\u001b[39m.\u001b[39mrandom()\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B643a5c50726f675c5347445f576f726b5c737061726b54657374/workspaces/sparkTest/development.ipynb#ch0000000vscode-remote?line=12'>13</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m x\u001b[39m*\u001b[39mx \u001b[39m+\u001b[39m y\u001b[39m*\u001b[39my \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://dev-container%2B643a5c50726f675c5347445f576f726b5c737061726b54657374/workspaces/sparkTest/development.ipynb#ch0000000vscode-remote?line=14'>15</a>\u001b[0m count \u001b[39m=\u001b[39m sc\u001b[39m.\u001b[39;49mparallelize(\u001b[39mrange\u001b[39;49m(\u001b[39m0\u001b[39;49m, num_samples))\u001b[39m.\u001b[39;49mfilter(inside)\u001b[39m.\u001b[39;49mcount()\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B643a5c50726f675c5347445f576f726b5c737061726b54657374/workspaces/sparkTest/development.ipynb#ch0000000vscode-remote?line=16'>17</a>\u001b[0m pi \u001b[39m=\u001b[39m \u001b[39m4\u001b[39m \u001b[39m*\u001b[39m count \u001b[39m/\u001b[39m num_samples\n\u001b[1;32m     <a href='vscode-notebook-cell://dev-container%2B643a5c50726f675c5347445f576f726b5c737061726b54657374/workspaces/sparkTest/development.ipynb#ch0000000vscode-remote?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(pi)\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/rdd.py:1237\u001b[0m, in \u001b[0;36mRDD.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1227'>1228</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcount\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1228'>1229</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1229'>1230</a>\u001b[0m \u001b[39m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1230'>1231</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1234'>1235</a>\u001b[0m \u001b[39m    3\u001b[39;00m\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1235'>1236</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1236'>1237</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m i: [\u001b[39msum\u001b[39;49m(\u001b[39m1\u001b[39;49m \u001b[39mfor\u001b[39;49;00m _ \u001b[39min\u001b[39;49;00m i)])\u001b[39m.\u001b[39;49msum()\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/rdd.py:1226\u001b[0m, in \u001b[0;36mRDD.sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1216'>1217</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msum\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1217'>1218</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1218'>1219</a>\u001b[0m \u001b[39m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1219'>1220</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1223'>1224</a>\u001b[0m \u001b[39m    6.0\u001b[39;00m\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1224'>1225</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1225'>1226</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(\u001b[39mlambda\u001b[39;49;00m x: [\u001b[39msum\u001b[39;49m(x)])\u001b[39m.\u001b[39;49mfold(\u001b[39m0\u001b[39;49m, operator\u001b[39m.\u001b[39;49madd)\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/rdd.py:1080\u001b[0m, in \u001b[0;36mRDD.fold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1075'>1076</a>\u001b[0m     \u001b[39myield\u001b[39;00m acc\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1076'>1077</a>\u001b[0m \u001b[39m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1077'>1078</a>\u001b[0m \u001b[39m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1078'>1079</a>\u001b[0m \u001b[39m# to the final reduce call\u001b[39;00m\n\u001b[0;32m-> <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1079'>1080</a>\u001b[0m vals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmapPartitions(func)\u001b[39m.\u001b[39;49mcollect()\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=1080'>1081</a>\u001b[0m \u001b[39mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/pyspark/rdd.py:950\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=940'>941</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=941'>942</a>\u001b[0m \u001b[39mReturn a list that contains all of the elements in this RDD.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=942'>943</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=946'>947</a>\u001b[0m \u001b[39mto be small, as all the data is loaded into the driver's memory.\u001b[39;00m\n\u001b[1;32m    <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=947'>948</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=948'>949</a>\u001b[0m \u001b[39mwith\u001b[39;00m SCCallSiteSync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext) \u001b[39mas\u001b[39;00m css:\n\u001b[0;32m--> <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=949'>950</a>\u001b[0m     sock_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonRDD\u001b[39m.\u001b[39;49mcollectAndServe(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jrdd\u001b[39m.\u001b[39;49mrdd())\n\u001b[1;32m    <a href='file:///~opt/bitnami/spark/python/pyspark/rdd.py?line=950'>951</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mlist\u001b[39m(_load_from_socket(sock_info, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1314'>1315</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1319'>1320</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1323'>1324</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1324'>1325</a>\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=329'>330</a>\u001b[0m             \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=330'>331</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=331'>332</a>\u001b[0m                 \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n\u001b[1;32m    <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=332'>333</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=333'>334</a>\u001b[0m         \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=334'>335</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[1;32m    <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=335'>336</a>\u001b[0m             \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name))\n\u001b[1;32m    <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=336'>337</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    <a href='file:///~opt/bitnami/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py?line=337'>338</a>\u001b[0m     \u001b[39mtype\u001b[39m \u001b[39m=\u001b[39m answer[\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/07 04:04:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:04:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:04:57 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:05:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:05:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:05:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:05:57 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:06:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:06:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:06:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:06:57 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:07:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:07:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:07:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:07:57 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:08:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:08:27 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:08:42 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:08:57 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/02/07 04:09:12 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "[Stage 0:>                                                          (0 + 0) / 2]\r"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "import random\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\", master=\"spark://host.docker.internal:7077\")\n",
    "# sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):\n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "573f8c4297b1e807d4e4d51bc4704711e08f3eef39a212b98b763079568da1b1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
